{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.48.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from QA_model.qa import (\n",
    "    load_csv_data_from_dirs,\n",
    "    init_model,\n",
    "    build_rag_data,\n",
    "    rag_search,\n",
    "    answer_question\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로딩된 CSV 행 개수: 1240\n",
      "RAG 데이터 개수: 1240\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/data/jung/src/datas\"  # CSV들이 들어있는 상위 폴더\n",
    "df_all = load_csv_data_from_dirs(base_dir=base_dir)   # 모든 CSV 통합 로드\n",
    "print(f\"로딩된 CSV 행 개수: {len(df_all)}\")\n",
    "\n",
    "\n",
    "rag_data = build_rag_data(df_all) #아직 임베딩 벡터가 없으므로 임의로 모든 항목을 리스트에 저장해서 해당 리스트를 db 처럼 사용중\n",
    "print(f\"RAG 데이터 개수: {len(rag_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EXAONE model: LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Qwen 사용시 질의 끝 혹은 코드 프롬프트에 한국어로 설명해달라는 프롬프트를 추가해야 함 아니면 중국어로만 나옴..\n",
    "# model_handle = init_model(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "# 또는\n",
    "model_handle = init_model(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")\n",
    "# 또는 (API 호출식)\n",
    "# model_handle = init_model(\"GPT-4o-mini\")   #GPT 이용 시에는 QA_model 폴더에 .env 파일 생성해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카카오뱅크는 2022년 2월에 비대면 주택담보대출 상품을 출시했습니다.\n"
     ]
    }
   ],
   "source": [
    "question_text = \"카카오는 언제 비대면 주택담보대출을 출시했나요? 정답은 한국어로 대답해줘\" \n",
    "answer = answer_question(question_text, rag_data, model_handle, top_k=3, use_bm25=False)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation 과 연결\n",
    "\n",
    "#### csv 파일에 있는 쿼리에 대해 모두 응답하면서 점수 측정 후 저장하는 구조 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SK하이닉스는 향후 3개년간 주주환원 정책을 공시하며, 연간 고정배당금을 1,500원으로 상향하고,  Free Cash Flow의 5% 재원을 재무 건전성 강화에 우선 활용한다고 밝혔다. 이는 메모리 업황 특성상 변동성이 큰 상황에서 내린 합리적인 결정이라고 판단된다.\n",
      "2. SK하이닉스의 2025~2027년 주주환원 정책에 대한 기준과 변경 사항을 정리한 테이블로, 배당금 및 Free Cash Flow 활용 방안이 포함되어 있습니다.\n",
      "3. SK하이닉스의 주요 재무 지표와 주식 관련 데이터를 나타내는 테이블로, KOSPI 지수, 시가총액, 외국인 지분율 등 다양한 정보를 포함하고 있습니다.\n",
      "#### 1. Retrieval Evaluation (20 points):\n",
      "\n",
      "1. **Do any of the retrieved contexts show strong similarity to the Ground Truth?** (5 points)  \n",
      "   [Yes] - Justification: The first retrieved context closely mirrors the Ground Truth, mentioning the increase in fixed dividends to 1,500 won and the prioritization of using 5% of Free Cash Flow for financial stability.\n",
      "\n",
      "2. **Do the retrieved contexts collectively capture essential information from the Ground Truth?** (5 points)  \n",
      "   [Yes] - Justification: The first context captures the key changes in the shareholder return policy, while the second context provides a summary table that likely includes relevant details, thus collectively covering the essential information.\n",
      "\n",
      "3. **Do the retrieved contexts sufficiently address the user’s question?** (4 points)  \n",
      "   [Yes] - Justification: The retrieved contexts provide direct answers to the user's question about the changes in SK hynix's shareholder return policy for 2025-2027, particularly in terms of dividend adjustments and financial strategies.\n",
      "\n",
      "4. **Are all retrieved contexts relevant to the Ground Truth or the user’s query?** (3 points)  \n",
      "   [No] - Justification: The third context, which contains various financial metrics and stock data, does not directly relate to the specific changes in the shareholder return policy and is less relevant to the user's query.\n",
      "\n",
      "5. **Does the combined length and number of retrieved contexts remain reasonable without overwhelming the user with excessive or irrelevant details?** (3 points)  \n",
      "   [Yes] - Justification: The number of retrieved contexts is manageable, and the relevant information is presented succinctly without overwhelming the user.\n",
      "\n",
      "**Total Retrieval Score**: 20 / 20\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. Generation Evaluation (30 points):\n",
      "\n",
      "1. **Is the final answer clearly relevant to the question and reflective of the user’s intent?** (5 points)  \n",
      "   [Yes] - Justification: The generated answer directly addresses the user's question about the changes in SK hynix's shareholder return policy for the specified years.\n",
      "\n",
      "2. **Is the answer factually correct and free from unsupported or inaccurate information?** (5 points)  \n",
      "   [Yes] - Justification: The answer accurately reflects the information provided in the retrieved contexts and aligns with the Ground Truth.\n",
      "\n",
      "3. **Does the answer include all essential points required by the question and the ground_truth_answer?** (5 points)  \n",
      "   [Yes] - Justification: The generated answer includes both the increase in fixed dividends and the allocation of Free Cash Flow, which are the key points from the Ground Truth.\n",
      "\n",
      "4. **Is the answer clear and concise, avoiding unnecessary repetition or ambiguity?** (5 points)  \n",
      "   [Yes] - Justification: The answer is structured clearly and concisely, presenting the information in a straightforward manner without unnecessary repetition.\n",
      "\n",
      "5. **Is the answer logically structured, consistent with the context, and free of contradictions?** (3 points)  \n",
      "   [Yes] - Justification: The answer is logically structured, presenting the changes in a clear order and maintaining consistency with the retrieved contexts.\n",
      "\n",
      "6. **Does the answer provide sufficient detail for the question without being excessive?** (3 points)  \n",
      "   [Yes] - Justification: The answer provides enough detail to fully address the question without going into excessive detail.\n",
      "\n",
      "7. **Does the answer provide proper citations or indications of the source when claims or data are referenced?** (2 points)  \n",
      "   [No] - Justification: The answer does not include citations or references to the sources of the information, which would enhance credibility.\n",
      "\n",
      "8. **Is the answer presented in a suitable format (list, table, short text, etc.) for the question?** (1 point)  \n",
      "   [Yes] - Justification: The answer is presented in a list format, which is appropriate for the information being conveyed.\n",
      "\n",
      "9. **Does the answer offer any helpful extra insights or context that enrich the user’s understanding (without deviating from factual correctness)?** (1 point)  \n",
      "   [Yes] - Justification: The mention of the rationale behind the decisions (considering market volatility) provides additional context that enriches the user's understanding.\n",
      "\n",
      "**Total Generation Score**: 27 / 30\n",
      "\n",
      "---\n",
      "\n",
      "### Final Output:\n",
      "**Total Score**: 47 / 50\n"
     ]
    }
   ],
   "source": [
    "from evaluation.GPT_evaluation import evaluate\n",
    "import pandas as pd\n",
    "from QA_model.qa import bm25_rag_search\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/data/jung/src/qa_validation_dataset_4_cleaning.csv\")\n",
    "\n",
    "query_list = df.apply(lambda row : [row[\"question\"], row[\"answer\"]], axis = 1).to_list()\n",
    "\n",
    "relevant_items = rag_search(\n",
    "        question=query_list[0][0],\n",
    "        data_list=rag_data,\n",
    "        top_k=3,\n",
    "        use_bm25=False\n",
    "        )\n",
    "\n",
    "context_parts = []\n",
    "for idx, item in enumerate(relevant_items, start=1):\n",
    "    if item[\"summary\"]:\n",
    "        context_parts.append(f\"{idx}. {item['summary']}\")\n",
    "    elif item[\"original_content\"]:\n",
    "        context_parts.append(f\"{idx}. {item['original_content']}\")\n",
    "retrieved_contexts = \"\\n\".join(context_parts)\n",
    "\n",
    "print(retrieved_contexts)\n",
    "\n",
    "generated_answer = answer_question(query_list[0][0], rag_data, model_handle, top_k=3, use_bm25=False)\n",
    "\n",
    "eval_score = evaluate(query_list[0][0], retrieved_contexts, query_list[0][1], generated_answer)\n",
    "\n",
    "print(eval_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추가 => 모델 클래스 분류 후 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from QA_model.qwen_model import QwenModel\n",
    "from QA_model.exaone_model import ExaoneModel\n",
    "from QA_model.gpt_model import GPTModel\n",
    "from QA_model.utils import load_csv_data_from_dirs, build_rag_data\n",
    "\n",
    "# 데이터 로드\n",
    "base_dir = \"/data/jung/src/datas\"\n",
    "df_all = load_csv_data_from_dirs(base_dir)\n",
    "rag_data = build_rag_data(df_all)\n",
    "\n",
    "# 모델 초기화 => 세개 중 선택\n",
    "# model1 = QwenModel()\n",
    "model2 = ExaoneModel()\n",
    "# model3 = GPTModel(model_name=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카카오는 2022년 2월에 비대면 주택담보대출 상품을 출시했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 질문 및 답변\n",
    "retrieval_results = \"카카오뱅크는 2022년 2월에 비대면 주택담보대출 상품을 출시했습니다.\"\n",
    "question = \"카카오는 언제 비대면 주택담보대출을 출시했나요?\"\n",
    "\n",
    "prompt = f\"주어진 문서를 반드시 참고하여 질문에 대한 답을 하세요.\\n\\n주어진 문서 : {retrieval_results}\\n질문 : {question}\"\n",
    "# print(model1.answering(prompt))\n",
    "# print(model2.answering(prompt))\n",
    "print(model2.answering(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluation.GPT_evaluation import evaluate\n",
    "from retrievals import bm25, dpr, ensemble\n",
    "from QA_model.qwen_model import QwenModel\n",
    "from QA_model.exaone_model import ExaoneModel\n",
    "from QA_model.gpt_model import GPTModel\n",
    "from DB.chromadb_storing import ChromaDB\n",
    "from langchain.schema import Document\n",
    "from utils import get_only_paragraphs, get_all_datas, create_documents\n",
    "\n",
    "# Retriever 초기화 => DB 는 생성 안되어서 이건 되는 사람이 테스트좀..\n",
    "topk = 3\n",
    "DPRRetriever = dpr(db, topk=topk)\n",
    "BM25Retriever = bm25(documents, topk=topk)\n",
    "retrievals = [DPRRetriever, BM25Retriever]\n",
    "weights = [0.5, 0.5]\n",
    "search_type = 'mmr'\n",
    "ensemble_retriever = ensemble(retrievals, topk=topk, weights=weights, search_type=search_type)\n",
    "\n",
    "retrievers = {\n",
    "    \"BM25\": BM25Retriever,\n",
    "    \"DPR\": DPRRetriever,\n",
    "    \"Ensemble\": ensemble_retriever\n",
    "}\n",
    "\n",
    "# 모델 초기화\n",
    "models = {\n",
    "    \"QwenModel\": QwenModel(),\n",
    "    \"ExaoneModel\": ExaoneModel(),\n",
    "    \"GPTModel\": GPTModel(model_name=\"gpt-4o-mini\")\n",
    "}\n",
    "\n",
    "# 평가 데이터 로드\n",
    "validation_df = pd.read_csv(\"/data/jung/src/qa_validation_dataset_4_cleaning.csv\")\n",
    "query_list = validation_df.apply(lambda row: [row[\"question\"], row[\"answer\"]], axis=1).to_list()\n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for retriever_name, retriever in retrievers.items():\n",
    "        for question, ground_truth in query_list:\n",
    "            # Retrieval\n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            retrieved_texts = [doc.page_content for doc in retrieved_docs]  # Retrieved 텍스트만 사용\n",
    "            retrieved_contexts = \"\\n\".join(retrieved_texts)\n",
    "\n",
    "            # 정답 생성\n",
    "            prompt = f\"주어진 문서를 반드시 참고하여 질문에 대한 답을 하세요.\\n\\n주어진 문서 : {retrieved_contexts}\\n질문 : {question}\"\n",
    "            generated_answer = model.answering(prompt)\n",
    "\n",
    "            # 평가\n",
    "            eval_score = evaluate(question, retrieved_contexts, ground_truth, generated_answer)\n",
    "\n",
    "            # 결과 저장\n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Retriever\": retriever_name,\n",
    "                \"Question\": question,\n",
    "                \"Ground Truth\": ground_truth,\n",
    "                \"Generated Answer\": generated_answer,\n",
    "                \"Retrieved Contexts\": retrieved_contexts,\n",
    "                \"Total Retrieval Score\": eval_score[\"retrieval_score\"],\n",
    "                \"Total Score\": eval_score[\"total_score\"]\n",
    "            })\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 CSV 파일 저장\n",
    "results_df.to_csv(\"/data/jung/src/evaluation_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Evaluation completed. Results saved to '/data/jung/src/evaluation_results.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

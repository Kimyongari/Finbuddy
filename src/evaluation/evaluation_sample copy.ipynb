{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT_evaluation import evaluate\n",
    "query = '한국의 수도는 어디니?'\n",
    "ground_truth_answer = '한국의 수도는 서울입니다.'\n",
    "retrieved_contexts = \"\"\"\n",
    "1. \"한국은 서울 수도로 지정하고 이를 중심으로 다양한 산업이 발전시켰다.\"\n",
    "2. \"서울은 수도이며, 한국에서 가장 유명한 도시이다.\"\n",
    "\"\"\"\n",
    "generated_answer = '한국의 수도는 서울이며, 서울에 대도시가 가장 많이 포진돼 있습니다.'\n",
    "\n",
    "answer = evaluate(query, retrieved_contexts, ground_truth_answer, generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### 1. Retrieval Evaluation (20 points):\n",
      "\n",
      "1. **Do any of the retrieved contexts show strong similarity to the Ground Truth?** (5 points)  \n",
      "   [Yes] - Justification: Both retrieved contexts explicitly mention that Seoul is the capital of Korea, which directly aligns with the Ground Truth answer.\n",
      "\n",
      "2. **Do the retrieved contexts collectively capture essential information from the Ground Truth?** (5 points)  \n",
      "   [Yes] - Justification: The retrieved contexts provide essential information about Seoul being the capital and its significance, which is crucial to understanding the Ground Truth.\n",
      "\n",
      "3. **Do the retrieved contexts sufficiently address the user’s question?** (4 points)  \n",
      "   [Yes] - Justification: The contexts directly answer the user's question about the capital of Korea, providing relevant information.\n",
      "\n",
      "4. **Are all retrieved contexts relevant to the Ground Truth or the user’s query?** (3 points)  \n",
      "   [Yes] - Justification: Both contexts are relevant as they pertain to the capital city of Korea and its characteristics.\n",
      "\n",
      "5. **Does the combined length and number of retrieved contexts remain reasonable without overwhelming the user with excessive or irrelevant details?** (3 points)  \n",
      "   [Yes] - Justification: The number of contexts is appropriate, and the information provided is concise and relevant, avoiding unnecessary details.\n",
      "\n",
      "**Total Retrieval Score**: 20 / 20\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. Generation Evaluation (30 points):\n",
      "\n",
      "1. **Is the final answer clearly relevant to the question and reflective of the user’s intent?** (5 points)  \n",
      "   [Yes] - Justification: The generated answer directly addresses the user's question about the capital of Korea, confirming that it is Seoul.\n",
      "\n",
      "2. **Is the answer factually correct and free from unsupported or inaccurate information?** (5 points)  \n",
      "   [Yes] - Justification: The answer correctly states that Seoul is the capital of Korea, which is accurate and supported by the retrieved contexts.\n",
      "\n",
      "3. **Does the answer include all essential points required by the question and the ground_truth_answer?** (5 points)  \n",
      "   [Yes] - Justification: The answer includes the essential point that Seoul is the capital, which is the main requirement of the question.\n",
      "\n",
      "4. **Is the answer clear and concise, avoiding unnecessary repetition or ambiguity?** (5 points)  \n",
      "   [Yes] - Justification: The answer is clear and concise, providing the necessary information without any ambiguity or repetition.\n",
      "\n",
      "5. **Is the answer logically structured, consistent with the context, and free of contradictions?** (3 points)  \n",
      "   [Yes] - Justification: The answer is logically structured and consistent with the retrieved contexts, with no contradictions present.\n",
      "\n",
      "6. **Does the answer provide sufficient detail for the question without being excessive?** (3 points)  \n",
      "   [Yes] - Justification: The answer provides sufficient detail by stating that Seoul is the capital and mentioning its status as a major city, which adds context without being excessive.\n",
      "\n",
      "7. **Does the answer provide proper citations or indications of the source when claims or data are referenced?** (2 points)  \n",
      "   [No] - Justification: The answer does not provide citations or references to the retrieved contexts, which would enhance credibility.\n",
      "\n",
      "8. **Is the answer presented in a suitable format (list, table, short text, etc.) for the question?** (1 point)  \n",
      "   [Yes] - Justification: The answer is presented in a clear text format, which is suitable for the question asked.\n",
      "\n",
      "9. **Does the answer offer any helpful extra insights or context that enrich the user’s understanding (without deviating from factual correctness)?** (1 point)  \n",
      "   [Yes] - Justification: The mention of Seoul being a major city provides additional context that enriches the user's understanding.\n",
      "\n",
      "**Total Generation Score**: 28 / 30\n",
      "\n",
      "---\n",
      "\n",
      "### Final Output:\n",
      "**Total Score**: 48 / 50\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackerthon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT_evaluation import evaluate\n",
    "query = 'What is the capital of France?'\n",
    "ground_truth_answer = 'The capital of France is Paris.'\n",
    "retrieved_contexts = \"\"\"\n",
    "1. \"Paris is the capital of France and known for its art and culture.\"\n",
    "2. \"France is a European country with Paris as its capital city.\"\n",
    "3. \"The Eiffel Tower is located in Paris, the capital of France.\"\n",
    "\"\"\"\n",
    "generated_answer = 'The capital of France is Paris, which is also famous for the Eiffel Tower.'\n",
    "\n",
    "answer = evaluate(query, retrieved_contexts, ground_truth_answer, generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### 1. Retrieval Evaluation (20 points):\n",
      "\n",
      "1. **Do any of the retrieved contexts show strong similarity to the Ground Truth?** (5 points)  \n",
      "   [Yes] - Justification: All retrieved contexts explicitly mention that Paris is the capital of France, which directly aligns with the Ground Truth answer.\n",
      "\n",
      "2. **Do the retrieved contexts collectively capture essential information from the Ground Truth?** (5 points)  \n",
      "   [Yes] - Justification: The retrieved contexts not only state that Paris is the capital but also provide additional context about Paris being known for its art and culture, and the Eiffel Tower, which enriches the understanding of the city.\n",
      "\n",
      "3. **Do the retrieved contexts sufficiently address the user’s question?** (4 points)  \n",
      "   [Yes] - Justification: The retrieved contexts directly answer the user's question by confirming that Paris is the capital of France, thus addressing the query effectively.\n",
      "\n",
      "4. **Are all retrieved contexts relevant to the Ground Truth or the user’s query?** (3 points)  \n",
      "   [Yes] - Justification: Each retrieved context is relevant as they all pertain to Paris and its status as the capital of France, providing pertinent information related to the query.\n",
      "\n",
      "5. **Does the combined length and number of retrieved contexts remain reasonable without overwhelming the user with excessive or irrelevant details?** (3 points)  \n",
      "   [Yes] - Justification: The number of retrieved contexts is appropriate, and they are concise, providing relevant information without overwhelming the user.\n",
      "\n",
      "**Total Retrieval Score**: 20 / 20\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. Generation Evaluation (30 points):\n",
      "\n",
      "1. **Is the final answer clearly relevant to the question and reflective of the user’s intent?** (5 points)  \n",
      "   [Yes] - Justification: The generated answer directly addresses the user's question about the capital of France, confirming that it is Paris.\n",
      "\n",
      "2. **Is the answer factually correct and free from unsupported or inaccurate information?** (5 points)  \n",
      "   [Yes] - Justification: The answer is factually correct; Paris is indeed the capital of France, and the mention of the Eiffel Tower is accurate.\n",
      "\n",
      "3. **Does the answer include all essential points required by the question and the ground_truth_answer?** (5 points)  \n",
      "   [Yes] - Justification: The answer includes the essential point that Paris is the capital of France, which is the core of the question.\n",
      "\n",
      "4. **Is the answer clear and concise, avoiding unnecessary repetition or ambiguity?** (5 points)  \n",
      "   [Yes] - Justification: The answer is clear and concise, stating the capital and adding a relevant detail without unnecessary repetition.\n",
      "\n",
      "5. **Is the answer logically structured, consistent with the context, and free of contradictions?** (3 points)  \n",
      "   [Yes] - Justification: The answer is logically structured, presenting the main fact first and then adding a related detail about the Eiffel Tower.\n",
      "\n",
      "6. **Does the answer provide sufficient detail for the question without being excessive?** (3 points)  \n",
      "   [Yes] - Justification: The answer provides a sufficient level of detail by mentioning the Eiffel Tower, which is a well-known landmark in Paris, without being excessive.\n",
      "\n",
      "7. **Does the answer provide proper citations or indications of the source when claims or data are referenced?** (2 points)  \n",
      "   [No] - Justification: The answer does not provide citations or references to the sources of information, which would enhance credibility.\n",
      "\n",
      "8. **Is the answer presented in a suitable format (list, table, short text, etc.) for the question?** (1 point)  \n",
      "   [Yes] - Justification: The answer is presented in a clear and straightforward text format, which is suitable for the question.\n",
      "\n",
      "9. **Does the answer offer any helpful extra insights or context that enrich the user’s understanding (without deviating from factual correctness)?** (1 point)  \n",
      "   [Yes] - Justification: The mention of the Eiffel Tower adds a helpful context that enriches the user's understanding of Paris.\n",
      "\n",
      "**Total Generation Score**: 28 / 30\n",
      "\n",
      "---\n",
      "\n",
      "### Final Output:\n",
      "**Total Score**: 48 / 50\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackerthon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
